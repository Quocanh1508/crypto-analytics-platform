# =============================================================================
# End-to-End Real-Time Crypto Analytics Platform
# Docker Compose Stack – Phase 0
# Executor: LocalExecutor (no Redis/Celery)
# Tested with: Docker Compose v2 (docker compose up -d)
# =============================================================================

x-airflow-common: &airflow-common
  image: apache/airflow:2.9.1-python3.11
  env_file: .env
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
    AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
    _PIP_ADDITIONAL_REQUIREMENTS: "great-expectations==0.18.8 dbt-postgres==1.8.2 psycopg2-binary==2.9.9 pandas>=2.2.0"
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/plugins:/opt/airflow/plugins
    - ./airflow/logs:/opt/airflow/logs
    - ./dbt_crypto:/opt/airflow/dbt_crypto
    - ./ge_tests:/opt/airflow/ge_tests
  depends_on:
    postgres:
      condition: service_healthy
  networks:
    - crypto-net

services:

  # ---------------------------------------------------------------------------
  # Zookeeper (required by Kafka)
  # ---------------------------------------------------------------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: crypto-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log
    mem_limit: 256m
    mem_reservation: 128m
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "bash -c '(echo > /dev/tcp/127.0.0.1/2181) 2>/dev/null'"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - crypto-net

  # ---------------------------------------------------------------------------
  # Kafka (single broker)
  # ---------------------------------------------------------------------------
  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: crypto-kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"          # internal
      - "29092:29092"        # external (host machine access)
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_HEAP_OPTS: "-Xmx768m -Xms256m"
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_SEGMENT_BYTES: 104857600
    volumes:
      - kafka-data:/var/lib/kafka/data
    mem_limit: 1g
    mem_reservation: 512m
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092"]
      interval: 20s
      timeout: 15s
      retries: 5
    networks:
      - crypto-net

  # ---------------------------------------------------------------------------
  # Python Producer (Binance WebSocket -> Kafka)
  # ---------------------------------------------------------------------------
  binance-producer:
    build:
      context: ./producer
    container_name: crypto-binance-producer
    env_file: .env
    environment:
      RUNNING_IN_DOCKER: "true"
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - crypto-net
    restart: on-failure:5
    profiles:
      - streaming

  # ---------------------------------------------------------------------------
  # Apache Spark (Structured Streaming)
  # ---------------------------------------------------------------------------
  spark-master:
    build:
      context: ./spark
    container_name: crypto-spark
    env_file: .env
    environment:
      RUNNING_IN_DOCKER: "true"
    command: >
      /opt/spark/bin/spark-submit
      --master local[*]
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262
      /app/spark/stream_job.py
    ports:
      - "4040:4040" # Spark Application WebUI
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      - ./spark:/app/spark
    networks:
      - crypto-net
    mem_limit: 1.5g
    mem_reservation: 512m
    restart: on-failure:3
    profiles:
      - spark-streaming

  # ---------------------------------------------------------------------------
  # Python Consumer (Kafka -> Postgres/MinIO)
  # ---------------------------------------------------------------------------
  binance-consumer:
    build:
      context: ./consumer
    container_name: crypto-binance-consumer
    env_file: .env
    environment:
      RUNNING_IN_DOCKER: "true"
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - crypto-net
    restart: on-failure:5
    profiles:
      - streaming

  # ---------------------------------------------------------------------------
  # MinIO (S3-compatible object store)
  # ---------------------------------------------------------------------------
  minio:
    image: minio/minio:RELEASE.2024-05-10T01-41-38Z
    container_name: crypto-minio
    ports:
      - "9000:9000"      # API
      - "9001:9001"      # Console UI
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    mem_limit: 512m
    mem_reservation: 256m
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 15s
      timeout: 10s
      retries: 5
    networks:
      - crypto-net

  # ---------------------------------------------------------------------------
  # MinIO init: create buckets after MinIO is healthy
  # ---------------------------------------------------------------------------
  minio-init:
    image: minio/mc:latest
    container_name: crypto-minio-init
    depends_on:
      minio:
        condition: service_healthy
    env_file: .env
    volumes:
      - ./init/minio/create_buckets.sh:/create_buckets.sh
    entrypoint: ["/bin/sh", "/create_buckets.sh"]
    networks:
      - crypto-net

  # ---------------------------------------------------------------------------
  # PostgreSQL (shared: airflow metadata + crypto_analytics)
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:16-alpine
    container_name: crypto-postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init/postgres:/docker-entrypoint-initdb.d
    mem_limit: 1g
    mem_reservation: 512m
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - crypto-net

  # ---------------------------------------------------------------------------
  # Airflow – DB migration (one-shot, runs airflow db migrate)
  # The /entrypoint prepends 'airflow' to the command, so 'db migrate' = 'airflow db migrate'
  # ---------------------------------------------------------------------------
  airflow-db-init:
    <<: *airflow-common
    container_name: crypto-airflow-db-init
    command: db migrate
    restart: on-failure

  # ---------------------------------------------------------------------------
  # Airflow – Admin user creation (runs after db-init)
  # bash is special-cased by /entrypoint (exec'd directly), so PATH is set correctly.
  # || true makes it idempotent (safe to re-run if user already exists)
  # ---------------------------------------------------------------------------
  airflow-user-init:
    <<: *airflow-common
    container_name: crypto-airflow-user-init
    command: >
      bash -c "airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@crypto.local || true"
    depends_on:
      airflow-db-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    networks:
      - crypto-net
    restart: on-failure:3

  # ---------------------------------------------------------------------------
  # Airflow – Webserver
  # ---------------------------------------------------------------------------
  airflow-webserver:
    <<: *airflow-common
    container_name: crypto-airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    mem_limit: 768m
    mem_reservation: 384m
    restart: unless-stopped
    depends_on:
      airflow-user-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 15s
      retries: 5

  # ---------------------------------------------------------------------------
  # Airflow – Scheduler (LocalExecutor runs tasks inside this process)
  # ---------------------------------------------------------------------------
  airflow-scheduler:
    <<: *airflow-common
    container_name: crypto-airflow-scheduler
    command: scheduler
    mem_limit: 768m
    mem_reservation: 384m
    restart: unless-stopped
    depends_on:
      airflow-user-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy

  # ---------------------------------------------------------------------------
  # Apache Superset
  # ---------------------------------------------------------------------------
  superset:
    image: apache/superset:4.0.2
    container_name: crypto-superset
    ports:
      - "8088:8088"
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      DATABASE_URL: postgresql+psycopg2://postgres:postgres@postgres:5432/superset
    volumes:
      - superset-data:/app/superset_home
      - ./superset/superset_config.py:/app/pythonpath/superset_config.py
    mem_limit: 1g
    mem_reservation: 512m
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      superset-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 30s
      timeout: 15s
      retries: 5
    networks:
      - crypto-net

  # ---------------------------------------------------------------------------
  # Superset – one-time init (DB upgrade + admin user + roles)
  # ---------------------------------------------------------------------------
  superset-init:
    image: apache/superset:4.0.2
    container_name: crypto-superset-init
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY}
      DATABASE_URL: postgresql+psycopg2://postgres:postgres@postgres:5432/superset
    volumes:
      - superset-data:/app/superset_home
      - ./superset/superset_config.py:/app/pythonpath/superset_config.py
    command: >
      bash -c "
        superset db upgrade &&
        superset fab create-admin
          --username ${SUPERSET_ADMIN_USERNAME}
          --firstname ${SUPERSET_ADMIN_FIRSTNAME}
          --lastname  ${SUPERSET_ADMIN_LASTNAME}
          --email ${SUPERSET_ADMIN_EMAIL}
          --password ${SUPERSET_ADMIN_PASSWORD} || true &&
        superset init
      "
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - crypto-net

# =============================================================================
# Volumes
# =============================================================================
volumes:
  zookeeper-data:
  zookeeper-logs:
  kafka-data:
  minio-data:
  postgres-data:
  superset-data:

# =============================================================================
# Networks
# =============================================================================
networks:
  crypto-net:
    driver: bridge
